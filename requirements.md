# スクラムガイド拡張パック RAG システム要求定義書

## 1. プロジェクト概要

### 1.1 目的

スクラムガイド拡張パック（2025.6版）を読み込んでRAG（Retrieval-Augmented Generation）システムを構築し、コマンドラインインターフェースを通じてスクラムに関する質問に回答するシステムを開発する。

### 1.2 対象文書

- **URL**: <https://scrumexpansion.org/ja/scrum-guide-expansion-pack/2025.6/pdf/scrum-guide-expansion-pack.ja.pdf>
- **形式**: PDF（HTMLより処理が簡単で一貫性があるため）
- **言語**: 日本語
- **更新頻度**: 一度構築後は更新しない（静的データ）

## 2. 技術要件

### 2.1 技術スタック

- **フレームワーク**: LangChain
- **ベクトルデータベース**: Chroma
- **埋め込みモデル**: OpenAI Embeddings
- **LLM**: OpenAI GPT-4
- **PDF処理**: LangChain PyPDFLoader
- **インターフェース**: コマンドライン（CLI）

### 2.2 環境設定

- **環境変数**: `ENV` (test/dev/prod)
- **設定ファイル**:
  - テスト環境: `.env.test`
  - 開発環境: `.env.dev`
  - 本番環境: `.env`
- **優先順位**: コマンドライン引数 > 環境変数 > デフォルト値

### 2.3 環境変数一覧

#### 必須変数

- **`OPENAI_API_KEY`**: OpenAI APIキー
- **`CHROMA_PERSIST_DIRECTORY`**: Chromaデータベースの保存ディレクトリ

#### 設定例

**`.env`**

```env
OPENAI_API_KEY=your_openai_api_key_here
CHROMA_PERSIST_DIRECTORY=.data/vector_db
```

## 3. 機能要件

### 3.1 コア機能

1. **文書読み込み**: PDFからテキストを抽出し、チャンクに分割
2. **ベクトル化**: テキストチャンクを埋め込みベクトルに変換
3. **インデックス作成**: Chromaデータベースにベクトルを保存
4. **検索・回答**: ユーザー質問に基づいて関連文書を検索し、LLMで回答生成

### 3.2 データ処理仕様

- **チャンクサイズ**: 1000文字（調整可能）
- **チャンクオーバーラップ**: 200文字
- **メタデータ**: ページ番号、セクション名、文書タイトル
- **エンコーディング**: UTF-8

### 3.3 CLI仕様

```bash
# ベクトルデータベース構築
python main.py build [--env {dev,test,prod}]

# 質問応答
python main.py query "質問内容" [--env {dev,test,prod}]

# ヘルプ
python main.py --help
```

### 3.4 環境切り替え仕様

- **コマンドライン引数**: `--env` オプションで環境を指定
- **環境変数**: `ENV` 環境変数でデフォルト環境を設定
- **設定ファイル**: 環境に応じて適切な `.env` ファイルを読み込み
- **優先順位**: `--env` オプション > `ENV` 環境変数 > デフォルト値（dev）

## 4. 非機能要件

### 4.1 パフォーマンス

- **文書処理時間**: PDF読み込み〜ベクトル化完了まで5分以内
- **検索応答時間**: 質問から回答まで10秒以内
- **メモリ使用量**: 8GB以下

### 4.2 セキュリティ

- **APIキー管理**: 環境変数での管理
- **データ保存**: ローカルファイルシステムのみ

### 4.3 品質

- **回答精度**: スクラムガイド拡張パックの内容に基づいた正確な回答
- **日本語対応**: 完全な日本語での質問・回答

## 5. アーキテクチャ

### 5.1 システム構成

```text
main.py
├── src/
│   ├── document_loader.py    # PDF読み込み
│   ├── vector_store.py       # Chroma操作
│   ├── rag_service.py        # RAG処理
│   └── cli.py               # CLI実装
├── data/
│   ├── raw/                 # 元PDFファイル
│   └── vector_db/           # Chromaデータベース
└── .env                     # 環境変数
```

### 5.2 処理フロー

1. **構築フェーズ**:
   PDF読み込み → テキスト抽出 → チャンク分割 → ベクトル化 → Chroma保存

2. **検索フェーズ**:
   質問入力 → ベクトル検索 → 関連文書取得 → LLM回答生成 → 結果出力

## 6. 実装方針

### 6.1 開発段階

1. **Phase 1**: PDF読み込み・テキスト抽出
2. **Phase 2**: ベクトル化・Chroma保存
3. **Phase 3**: RAG検索・回答生成
4. **Phase 4**: CLI実装・統合テスト

### 6.2 エラーハンドリング

- PDF読み込みエラー: ファイル存在確認、権限チェック
- API接続エラー: リトライ機能、タイムアウト設定
- 検索エラー: 空結果の適切な処理

## 7. テスト要件

### 7.1 単体テスト

- 各モジュールの機能テスト
- エラーケースのテスト
- モックを使用した外部APIテスト

### 7.2 統合テスト

- エンドツーエンドの質問応答テスト
- 日本語テキストの正確性テスト

## 8. 運用要件

### 8.1 環境設定

- Python 3.13以上
- OpenAI APIキーの設定（各環境の `.env` ファイルに記載）
- 十分なディスク容量（ベクトルDB用）
- 環境変数 `ENV` の設定（オプション、デフォルト: dev）

### 8.2 使用方法

1. 依存関係インストール: `uv sync`
2. 環境変数設定: 各環境の `.env` ファイル作成（`.env.dev`, `.env.test`, `.env`）
3. データベース構築: `python main.py build --env dev`
4. 質問実行: `python main.py query "質問内容" --env dev`

## 9. 制約事項

### 9.1 技術制約

- 一度構築したベクトルデータベースは更新しない
- ローカル環境でのみ動作（クラウド非対応）
- セキュリティ・パフォーマンスはPoCレベル

### 9.2 機能制約

- スクラムガイド拡張パックの内容のみに限定
- リアルタイム更新なし
- 複数文書の同時検索なし

## 10. 成功基準

- [ ] PDFからテキストを正確に抽出できる
- [ ] ベクトルデータベースを正常に構築できる
- [ ] 日本語での質問に適切に回答できる
- [ ] CLIから簡単に操作できる
- [ ] スクラムガイド拡張パックの内容に基づいた正確な回答が得られる
